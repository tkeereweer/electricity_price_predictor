{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import itertools\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "gsOuL5n6FrDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######### data import and processing\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "file_path = '/content/Data_Combined_2023-2024_daily.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Drop the unnecessary index column\n",
        "data.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "\n",
        "# Convert date column to datetime format and set as index\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "data.set_index('Date', inplace=True)\n",
        "\n",
        "# Drop the Gas Demand column\n",
        "data.drop(columns=['Gas Demand'], inplace=True)\n",
        "\n",
        "# Handle missing values\n",
        "data['Gas Price'].fillna(method='ffill', inplace=True)\n",
        "data['interconn_fra'].fillna(method='ffill', inplace=True)\n",
        "\n",
        "# Forward fill first, then backward fill\n",
        "data['CO2_Value'].fillna(method='ffill', inplace=True)\n",
        "data['CO2_Value'].fillna(method='bfill', inplace=True)\n",
        "\n",
        "data.rename(columns={'Electricity Demand': 'Electricity_Demand', 'Gas Price': 'Gas_Price', 'Electricity Price': 'Electricity_Price'}, inplace=True)\n",
        "\n",
        "\n",
        "# Check for remaining missing values\n",
        "missing_values_after = data.isnull().sum()\n",
        "print(\"Missing values after cleaning:\")\n",
        "print(missing_values_after)\n",
        "\n"
      ],
      "metadata": {
        "id": "c4BJPA1bFsAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############ BASE MODEL\n",
        "\n",
        "# Define the target and independent variables\n",
        "target = 'Electricity_Price'\n",
        "independent_vars = data.columns.drop(target).tolist()\n",
        "X = data[independent_vars]\n",
        "y = data[target]\n",
        "\n",
        "# Fit the ARIMAX model\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "order = (1, 1, 1)  # ARIMA order (p, d, q), these should be tuned based on your data\n",
        "seasonal_order = (0, 0, 0, 0)  # No seasonality for ARIMAX\n",
        "\n",
        "model = SARIMAX(y, exog=X, order=order, seasonal_order=seasonal_order)\n",
        "results = model.fit(disp=False)\n",
        "\n",
        "# Summary of the model\n",
        "print(results.summary())\n",
        "\n",
        "# Model diagnostics and forecasting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Diagnostics\n",
        "results.plot_diagnostics(figsize=(15, 12))\n",
        "plt.show()\n",
        "\n",
        "# Forecast\n",
        "forecast_steps = 30  # Number of days to forecast\n",
        "forecast = results.get_forecast(steps=forecast_steps, exog=X[-forecast_steps:])\n",
        "\n",
        "# Corrected forecast index generation\n",
        "forecast_index = pd.date_range(start=X.index[-1] + pd.Timedelta(days=1), periods=forecast_steps)\n",
        "forecast_mean = forecast.predicted_mean\n",
        "forecast_ci = forecast.conf_int()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(y, label='Observed')\n",
        "plt.plot(forecast_index, forecast_mean, label='Forecast')\n",
        "plt.fill_between(forecast_index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], color='k', alpha=0.1)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rUCnp7t1cYdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prophet import Prophet\n",
        "from prophet.diagnostics import cross_validation, performance_metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Prepare the data\n",
        "data_prophet = data.reset_index().rename(columns={'Date': 'ds', 'Electricity_Price': 'y'})\n",
        "\n",
        "# Create lagged features, moving averages, and interaction terms\n",
        "data_prophet['lag_1'] = data_prophet['y'].shift(1)\n",
        "data_prophet['lag_7'] = data_prophet['y'].shift(7)\n",
        "data_prophet['ma_7'] = data_prophet['y'].rolling(window=7).mean()\n",
        "data_prophet['ma_30'] = data_prophet['y'].rolling(window=30).mean()\n",
        "\n",
        "# Additional interaction terms\n",
        "data_prophet['interaction_1'] = data_prophet['Gas_Price'] * data_prophet['CO2_Value']\n",
        "data_prophet['interaction_2'] = data_prophet['Temperature'] * data_prophet['Electricity_Demand']\n",
        "data_prophet['interaction_3'] = data_prophet['Gas Price'] * data_prophet['Electricity_Demand']\n",
        "data_prophet['interaction_4'] = data_prophet['Temperature'] * data_prophet['Gas_Price']\n",
        "\n",
        "# Drop NaN values created by shifting and rolling calculations\n",
        "data_prophet.dropna(inplace=True)\n",
        "\n",
        "# Split into training and testing sets\n",
        "train_size = int(len(data_prophet) * 0.8)\n",
        "train_data, test_data = data_prophet[:train_size], data_prophet[train_size:]\n",
        "\n",
        "# Hyperparameter grid (extended)\n",
        "param_grid = {\n",
        "    'seasonality_mode': ['additive', 'multiplicative'],\n",
        "    'yearly_seasonality': [True, False],\n",
        "    'weekly_seasonality': [True, False],\n",
        "    'daily_seasonality': [False],  # Typically, daily seasonality is not necessary for daily data\n",
        "    'seasonality_prior_scale': [0.01, 0.1, 1.0, 10.0],\n",
        "    'changepoint_prior_scale': [0.001, 0.01, 0.1, 0.5, 1.0],\n",
        "}\n",
        "\n",
        "# List of regressors (defined globally)\n",
        "regressors = ['Temperature', 'Electricity_Demand', 'Gas_Price', 'interconn_fra', 'CO2_Value'] + \\\n",
        "             ['lag_1', 'lag_7', 'ma_7', 'ma_30', 'interaction_1', 'interaction_2', 'interaction_3', 'interaction_4']\n",
        "\n",
        "# Function to train and evaluate a model with given parameters\n",
        "def evaluate_prophet(params):\n",
        "    model = Prophet(\n",
        "        seasonality_mode=params['seasonality_mode'],\n",
        "        yearly_seasonality=params['yearly_seasonality'],\n",
        "        weekly_seasonality=params['weekly_seasonality'],\n",
        "        daily_seasonality=params['daily_seasonality'],\n",
        "        seasonality_prior_scale=params['seasonality_prior_scale'],\n",
        "        changepoint_prior_scale=params['changepoint_prior_scale']\n",
        "    )\n",
        "    for regressor in regressors:\n",
        "        model.add_regressor(regressor)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(train_data)\n",
        "\n",
        "    # Perform cross-validation\n",
        "    df_cv = cross_validation(model, initial='180 days', period='90 days', horizon='180 days')\n",
        "    df_p = performance_metrics(df_cv)\n",
        "\n",
        "    return df_p['rmse'].mean()\n",
        "\n",
        "# Perform grid search\n",
        "best_params = None\n",
        "best_score = float('inf')\n",
        "for seasonality_mode in param_grid['seasonality_mode']:\n",
        "    for yearly_seasonality in param_grid['yearly_seasonality']:\n",
        "        for weekly_seasonality in param_grid['weekly_seasonality']:\n",
        "            for daily_seasonality in param_grid['daily_seasonality']:\n",
        "                for seasonality_prior_scale in param_grid['seasonality_prior_scale']:\n",
        "                    for changepoint_prior_scale in param_grid['changepoint_prior_scale']:\n",
        "                        params = {\n",
        "                            'seasonality_mode': seasonality_mode,\n",
        "                            'yearly_seasonality': yearly_seasonality,\n",
        "                            'weekly_seasonality': weekly_seasonality,\n",
        "                            'daily_seasonality': daily_seasonality,\n",
        "                            'seasonality_prior_scale': seasonality_prior_scale,\n",
        "                            'changepoint_prior_scale': changepoint_prior_scale\n",
        "                        }\n",
        "                        score = evaluate_prophet(params)\n",
        "                        if score < best_score:\n",
        "                            best_score = score\n",
        "                            best_params = params\n",
        "                        print(f'Params: {params}, RMSE: {score}')\n",
        "\n",
        "print(f'Best Params: {best_params}, Best RMSE: {best_score}')\n",
        "\n",
        "# Train the final model with the best parameters\n",
        "model2 = Prophet(\n",
        "    seasonality_mode=best_params['seasonality_mode'],\n",
        "    yearly_seasonality=best_params['yearly_seasonality'],\n",
        "    weekly_seasonality=best_params['weekly_seasonality'],\n",
        "    daily_seasonality=best_params['daily_seasonality'],\n",
        "    seasonality_prior_scale=best_params['seasonality_prior_scale'],\n",
        "    changepoint_prior_scale=best_params['changepoint_prior_scale']\n",
        ")\n",
        "for regressor in regressors:\n",
        "    model2.add_regressor(regressor)\n",
        "\n",
        "# Fit the model\n",
        "model2.fit(train_data)\n",
        "\n",
        "# Make predictions\n",
        "future = model2.make_future_dataframe(periods=len(test_data))\n",
        "for regressor in regressors:\n",
        "    future[regressor] = data_prophet[regressor]\n",
        "\n",
        "# Fill NaN values in future with the last available values\n",
        "for col in regressors:\n",
        "    future[col].fillna(method='ffill', inplace=True)\n",
        "    future[col].fillna(method='bfill', inplace=True)\n",
        "\n",
        "forecast = model2.predict(future)\n",
        "\n",
        "# Evaluate the performance\n",
        "y_pred = forecast['yhat'][train_size:]\n",
        "y_true = test_data['y']\n",
        "\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
        "print(f'Mean Absolute Percentage Error (MAPE): {mape}')\n",
        "\n",
        "# Plot the forecast\n",
        "fig = model2.plot(forecast)\n",
        "plt.show()\n",
        "\n",
        "# Plot the forecast components\n",
        "fig2 = model2.plot_components(forecast)\n",
        "plt.show()\n",
        "\n",
        "# Perform cross-validation on the final model\n",
        "df_cv = cross_validation(model2, initial='180 days', period='90 days', horizon='180 days')\n",
        "df_p = performance_metrics(df_cv)\n",
        "\n",
        "print(df_p[['horizon', 'mae', 'mse', 'rmse', 'mape']])\n",
        "\n",
        "# Plot cross-validation results\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.plot(df_cv['ds'], df_cv['y'], 'k.', label='Actual')\n",
        "ax.plot(df_cv['ds'], df_cv['yhat'], 'b-', label='Predicted')\n",
        "ax.fill_between(df_cv['ds'].dt.to_pydatetime(), df_cv['yhat_lower'], df_cv['yhat_upper'], color='blue', alpha=0.2)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "##model is still better than model 1 and 2 and the best parameters are\n",
        "#Best Params: {'seasonality_mode': 'multiplicative', 'yearly_seasonality': False, 'weekly_seasonality': True, 'daily_seasonality': False, 'seasonality_prior_scale': 0.1, 'changepoint_prior_scale': 0.01}\n",
        "\n",
        "\n",
        "#lets try arimax again\n"
      ],
      "metadata": {
        "id": "ueB8O64Tcizv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########## THIS USED TO FIND THE BEST PARAMETERS\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Prepare the data\n",
        "data_prophet = data.reset_index().rename(columns={'Date': 'ds', 'Electricity_Price': 'y'})\n",
        "\n",
        "# Create lagged features, moving averages, and interaction terms\n",
        "data_prophet['lag_1'] = data_prophet['y'].shift(1)\n",
        "data_prophet['lag_7'] = data_prophet['y'].shift(7)\n",
        "data_prophet['ma_7'] = data_prophet['y'].rolling(window=7).mean()\n",
        "data_prophet['ma_30'] = data_prophet['y'].rolling(window=30).mean()\n",
        "\n",
        "# Additional interaction terms\n",
        "data_prophet['interaction_1'] = data_prophet['Gas_Price'] * data_prophet['CO2_Value']\n",
        "data_prophet['interaction_2'] = data_prophet['Temperature'] * data_prophet['Electricity_Demand']\n",
        "data_prophet['interaction_3'] = data_prophet['Gas_Price'] * data_prophet['Electricity_Demand']\n",
        "data_prophet['interaction_4'] = data_prophet['Temperature'] * data_prophet['Gas_Price']\n",
        "\n",
        "# Drop NaN values created by shifting and rolling calculations\n",
        "data_prophet.dropna(inplace=True)\n",
        "\n",
        "# Define the target and independent variables\n",
        "target = 'y'\n",
        "independent_vars = ['Temperature', 'Electricity_Demand', 'Gas_Price', 'interconn_fra', 'CO2_Value',\n",
        "                    'lag_1', 'lag_7', 'ma_7', 'ma_30', 'interaction_1', 'interaction_2', 'interaction_3', 'interaction_4']\n",
        "X = data_prophet[independent_vars]\n",
        "y = data_prophet[target]\n",
        "\n",
        "# Split into training and testing sets\n",
        "train_size = int(len(data_prophet) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Define a larger grid search for ARIMA order and seasonal order\n",
        "p = d = q = range(0, 3)\n",
        "seasonal_p = seasonal_d = seasonal_q = range(0, 2)\n",
        "seasonal_period = [0, 7, 30]\n",
        "\n",
        "# Generate all different combinations of (p, d, q) and seasonal (P, D, Q, s)\n",
        "pdq = list(itertools.product(p, d, q))\n",
        "seasonal_pdq = list(itertools.product(seasonal_p, seasonal_d, seasonal_q, seasonal_period))\n",
        "\n",
        "# Function to evaluate ARIMAX model\n",
        "def evaluate_arimax(order, seasonal_order, exog, endog):\n",
        "    try:\n",
        "        model = SARIMAX(endog, exog=exog, order=order, seasonal_order=seasonal_order)\n",
        "        results = model.fit(disp=False)\n",
        "        y_pred = results.predict(start=len(y_train), end=len(y_train) + len(y_test) - 1, exog=X_test)\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        return rmse\n",
        "    except:\n",
        "        return float('inf')\n",
        "\n",
        "# Perform grid search\n",
        "best_score = float('inf')\n",
        "best_params = None\n",
        "for param in pdq:\n",
        "    for param_seasonal in seasonal_pdq:\n",
        "        score = evaluate_arimax(param, param_seasonal, X_train, y_train)\n",
        "        if score < best_score:\n",
        "            best_score = score\n",
        "            best_params = (param, param_seasonal)\n",
        "        print(f'ARIMA{param}x{param_seasonal}, RMSE: {score}')\n",
        "\n",
        "print(f'Best Params: ARIMA{best_params[0]}x{best_params[1]}, Best RMSE: {best_score}')\n",
        "\n",
        "# Train the final model with the best parameters\n",
        "model3 = SARIMAX(y, exog=X, order=best_params[0], seasonal_order=best_params[1])\n",
        "results3 = model3.fit(disp=False)\n",
        "\n",
        "# Summary of the model\n",
        "print(results3.summary())\n",
        "\n",
        "# Model diagnostics and forecasting\n",
        "results3.plot_diagnostics(figsize=(15, 12))\n",
        "plt.show()\n",
        "\n",
        "# Forecast\n",
        "forecast_steps = 30  # Number of days to forecast\n",
        "forecast = results3.get_forecast(steps=forecast_steps, exog=X[-forecast_steps:])\n",
        "\n",
        "# Corrected forecast index generation\n",
        "forecast_index = pd.date_range(start=X.index[-1] + pd.Timedelta(days=1), periods=forecast_steps)\n",
        "forecast_mean = forecast.predicted_mean\n",
        "forecast_ci = forecast.conf_int()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(y, label='Observed')\n",
        "plt.plot(forecast_index, forecast_mean, label='Forecast')\n",
        "plt.fill_between(forecast_index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], color='k', alpha=0.1)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the performance on the test set\n",
        "y_pred = results3.predict(start=len(y_train), end=len(y_train) + len(y_test) - 1, exog=X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
        "print(f'Mean Absolute Percentage Error (MAPE): {mape}')\n"
      ],
      "metadata": {
        "id": "ND75t3lgdeLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########FINAL MODEL SARIMAX\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Define the target and independent variables\n",
        "target = 'Electricity_Price'\n",
        "independent_vars = data.columns.drop(target).tolist()\n",
        "X = data[independent_vars]\n",
        "y = data[target]\n",
        "\n",
        "\n",
        "# Prepare the data (assuming data is already loaded in a DataFrame called 'data')\n",
        "data_prophet = data.reset_index().rename(columns={'Date': 'ds', 'Electricity_Price': 'y'})\n",
        "\n",
        "\n",
        "\n",
        "# Create lagged features, moving averages, and interaction terms\n",
        "data_prophet['lag_1'] = data_prophet['y'].shift(1)\n",
        "data_prophet['lag_7'] = data_prophet['y'].shift(7)\n",
        "data_prophet['ma_7'] = data_prophet['y'].rolling(window=7).mean()\n",
        "data_prophet['ma_30'] = data_prophet['y'].rolling(window=30).mean()\n",
        "\n",
        "data_prophet['interaction_1'] = data_prophet['Gas_Price'] * data_prophet['CO2_Value']\n",
        "data_prophet['interaction_2'] = data_prophet['Temperature'] * data_prophet['Electricity_Demand']\n",
        "data_prophet['interaction_3'] = data_prophet['Gas_Price'] * data_prophet['Electricity_Demand']\n",
        "data_prophet['interaction_4'] = data_prophet['Temperature'] * data_prophet['Gas_Price']\n",
        "\n",
        "data_prophet.dropna(inplace=True)\n",
        "\n",
        "# Define the target and independent variables\n",
        "target = 'y'\n",
        "independent_vars = ['Temperature', 'Electricity_Demand', 'Gas_Price', 'interconn_fra', 'CO2_Value',\n",
        "                    'lag_1', 'lag_7', 'ma_7', 'ma_30', 'interaction_1', 'interaction_2', 'interaction_3', 'interaction_4']\n",
        "X = data_prophet[independent_vars]\n",
        "y = data_prophet[target]\n",
        "\n",
        "# Split into training and testing sets\n",
        "train_size = int(len(data_prophet) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Best parameters from previous grid search\n",
        "best_order = (0, 0, 0)\n",
        "best_seasonal_order = (1, 0, 0, 7)\n",
        "\n",
        "# Train the final model with the best parameters\n",
        "model3 = SARIMAX(y_train, exog=X_train, order=best_order, seasonal_order=best_seasonal_order)\n",
        "results3 = model3.fit(disp=False)\n",
        "\n",
        "# Summary of the model\n",
        "print(results3.summary())\n",
        "\n",
        "# Model diagnostics\n",
        "results3.plot_diagnostics(figsize=(15, 12))\n",
        "plt.show()\n",
        "\n",
        "# Forecast future values\n",
        "forecast_steps = len(y_test)  # Number of steps to forecast\n",
        "forecast = results3.get_forecast(steps=forecast_steps, exog=X_test)\n",
        "\n",
        "# Generate forecast index\n",
        "forecast_index = y_test.index\n",
        "forecast_mean = forecast.predicted_mean\n",
        "forecast_ci = forecast.conf_int()\n",
        "\n",
        "# Plot forecast\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(y_train, label='Train')\n",
        "plt.plot(y_test, label='Test')\n",
        "plt.plot(forecast_index, forecast_mean, label='Forecast')\n",
        "plt.fill_between(forecast_index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], color='k', alpha=0.1)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the performance on the test set\n",
        "y_pred = results3.predict(start=len(y_train), end=len(y_train) + len(y_test) - 1, exog=X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
        "print(f'Mean Absolute Percentage Error (MAPE): {mape}')\n"
      ],
      "metadata": {
        "id": "rcMqWiBnFswZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mape = 0.17347834046876742\n",
        "accuracy = 100 - (mape * 100)\n",
        "print(f'Accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "Cx8-FNJjcDaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "TV8s85jAcHFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Assuming 'model3' is your ARIMAX model fitted with the best parameters\n",
        "model3_file_path = '/content/drive/My Drive/Electricity_Price_model.pkl'\n",
        "joblib.dump(results3, model3_file_path)\n",
        "print(f'Model saved to {model3_file_path}')\n"
      ],
      "metadata": {
        "id": "CaSnHPVScJyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models for independent variables"
      ],
      "metadata": {
        "id": "N0vQlTQ_7Mei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rename columns\n",
        "data.rename(columns={'Electricity Demand': 'Electricity_Demand', 'Gas Price': 'Gas_Price' }, inplace=True)"
      ],
      "metadata": {
        "id": "-cCt6rF_7SAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_lagged_dataframe(data, var_name):\n",
        "    # Initialize the dataframe with the original column\n",
        "    lagged_df = data[[var_name]].copy()\n",
        "\n",
        "    # Create lagged columns from lag1 to lag15\n",
        "    for i in range(1, 16):\n",
        "        lagged_df[f'{var_name}_lag{i}'] = data[var_name].shift(i)\n",
        "\n",
        "    # Remove the first 15 rows that contain NaN values\n",
        "    lagged_df = lagged_df.dropna()\n",
        "\n",
        "    return lagged_df"
      ],
      "metadata": {
        "id": "BHU_GVWG7bxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CO2_Value_lags = create_lagged_dataframe(data, 'Electricity_Demand')"
      ],
      "metadata": {
        "id": "5gLkYkGp7fwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write csv\n",
        "CO2_Value_lags.to_csv('Electricity_Demand_lags.csv')"
      ],
      "metadata": {
        "id": "XNnc4A4G7kMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "def train_and_plot_regression(data, var_name):\n",
        "    # Create the lagged dataframe\n",
        "    lagged_df = create_lagged_dataframe(data, var_name)\n",
        "\n",
        "    # Prepare the features and target variable\n",
        "    X = lagged_df.drop(columns=[var_name])\n",
        "    y = lagged_df[var_name]\n",
        "\n",
        "    # Determine the split index for 80/20 split\n",
        "    split_index = int(len(X) * 0.8)\n",
        "    X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
        "    y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
        "    test_dates = y_test.index  # Keeping the date index for plotting\n",
        "\n",
        "    # Initialize and train the linear regression model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict the target variable on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Plotting the results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(test_dates, y_test, label='Actual', color='blue', marker='o')\n",
        "    plt.plot(test_dates, y_pred, label='Predicted', color='red', linestyle='--', marker='x')\n",
        "    plt.title(f'Actual vs Predicted {var_name} Values Over Time')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel(f'{var_name}')\n",
        "    plt.legend()\n",
        "    plt.xticks(rotation=45)  # Rotate date labels for better readability\n",
        "    plt.tight_layout()  # Adjust layout to make room for label rotation\n",
        "    plt.show()\n",
        "\n",
        "    # Optional: Return model, scores, or coefficients if needed\n",
        "    return model, X_train, X_test, y_train, y_test, y_pred"
      ],
      "metadata": {
        "id": "NNqyztmr7mOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, X_train, X_test, y_train, y_test, y_pred = train_and_plot_regression(data, 'CO2_Value')"
      ],
      "metadata": {
        "id": "dmbcMEpZ7qm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for feature, coef in zip(X_train.columns, model.coef_):\n",
        "    print(f\"{feature}: {coef}\")"
      ],
      "metadata": {
        "id": "RlC_0p727w6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "model_file_path = '/content/drive/My Drive/CO2_Value_model.pkl'\n",
        "joblib.dump(model, model_file_path)\n",
        "print(f'Model saved to {model_file_path}')"
      ],
      "metadata": {
        "id": "sBdefKsF7zwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import timedelta\n",
        "import pandas as pd\n",
        "from joblib import load\n",
        "\n",
        "def predict_future_values(var_names, end_date, data):\n",
        "    for var_name in var_names:\n",
        "        # Load the model for the specified variable\n",
        "        model = load(f\"{var_name}_model.pkl\")\n",
        "        var_lags = pd.read_csv(f'{var_name}_lags.csv', index_col='Date', parse_dates=True)\n",
        "\n",
        "        # Convert end_date to pandas Timestamp\n",
        "        end_date = pd.to_datetime(end_date)\n",
        "\n",
        "        # Make sure the index is in datetime format and the DataFrame is sorted by index\n",
        "        var_lags.index = pd.to_datetime(var_lags.index)\n",
        "        var_lags = var_lags.sort_index()\n",
        "\n",
        "        # Start predicting from the day after the latest date in var_lags\n",
        "        current_date = var_lags.index.max() + timedelta(days=1)\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            # Extract the last 15 entries to calculate lags\n",
        "            last_entries = var_lags.tail(15)\n",
        "            new_row = {f'{var_name}_lag{i}': last_entries[var_name].shift(i-1).iloc[-1] for i in range(1, 16)}\n",
        "\n",
        "            # Prepare features and predict\n",
        "            features = [new_row[f'{var_name}_lag{i}'] for i in range(1, 16)]\n",
        "            predicted_value = model.predict([features])[0]\n",
        "\n",
        "            # Append new row to DataFrame with current_date as the index\n",
        "            new_row[var_name] = predicted_value\n",
        "            var_lags.loc[current_date] = new_row\n",
        "\n",
        "            # Add the prediction to the data DataFrame\n",
        "            if var_name not in data.columns:\n",
        "                data[var_name] = pd.NA\n",
        "            data.loc[current_date, var_name] = predicted_value\n",
        "\n",
        "            # Increment the date by one day\n",
        "            current_date += timedelta(days=1)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "5R3AfKTa70i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "end_date = '2024-06-30'\n",
        "var_names = ['Temperature', 'Electricity_Demand', 'Gas_Price', 'CO2_Value', 'interconn_fra']\n",
        "data_predicted = predict_future_values(var_names, end_date, data)"
      ],
      "metadata": {
        "id": "A2JFQF0d7-l3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}